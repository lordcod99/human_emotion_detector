{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotionfinder_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1-fMaGYSFp1gkkvIVTbrjbXBFF1CqiG-T",
      "authorship_tag": "ABX9TyMsH+adUVqXNVg3HdAiXD1e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lordcod99/human_emotion_detector/blob/main/Emotionfinder_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5NYUFRQ-Wx8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import tensorflow\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator \n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization, Conv2D, MaxPooling2D, concatenate \n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.regularizers import l1, l2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# below is to check if we provided complete gpu usage from colab for better training if already used comment out install commands \n",
        "\n",
        "# memory footprint support libraries/code\n",
        "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "# !pip install gputil\n",
        "# !pip install psutil\n",
        "# !pip install humanize\n",
        "# import psutil\n",
        "# import humanize\n",
        "# import os\n",
        "# import GPUtil as GPU\n",
        "# GPUs = GPU.getGPUs()\n",
        "# # XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "# gpu = GPUs[0]\n",
        "# def printm():\n",
        "#  process = psutil.Process(os.getpid())\n",
        "#  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "#  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "# printm() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RUE9aGmMV-s",
        "outputId": "40065efd-5bdd-4168-d966-f7462ba5fd02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gputil in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.4 GB  | Proc size: 96.2 MB\n",
            "GPU RAM Free: 15109MB | Used: 0MB | Util   0% | Total 15109MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRrMrBGsMzgx",
        "outputId": "dc6421ff-b7dd-4532-9654-f91470fe28ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/drive/MyDrive/training/fer2013.csv\")\n",
        "# data.head()"
      ],
      "metadata": {
        "id": "XcJ-QS0yA7sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = [] #train data image pixels\n",
        "y_train = []  #train data labbles\n",
        "X_test = []  #test data image pixels\n",
        "y_test = []  #test data labbels \n",
        "\n",
        "for index, row in data.iterrows():\n",
        "    k = row['pixels'].split(\" \")\n",
        "    if row['Usage'] == 'Training':\n",
        "        X_train.append(np.array(k))\n",
        "        y_train.append(row['emotion'])\n",
        "    elif row['Usage'] == 'PublicTest':\n",
        "        X_test.append(np.array(k))\n",
        "        y_test.append(row['emotion'])\n",
        "\n",
        "# data set images are in from of string so making them to numbers\n",
        "X_train = np.array(X_train, dtype = 'uint8')\n",
        "y_train = np.array(y_train, dtype = 'uint8')\n",
        "X_test = np.array(X_test, dtype = 'uint8')\n",
        "y_test = np.array(y_test, dtype = 'uint8')\n",
        "\n",
        "#cotogoring data to 7 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral \n",
        "y_train= to_categorical(y_train, num_classes=7)\n",
        "y_test = to_categorical(y_test, num_classes=7)\n",
        "\n",
        "# images are 48 * 48 pixels and reshape data to 4d tensor , 1 at end says it\n",
        "# is a gray scale image\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)\n",
        "\n"
      ],
      "metadata": {
        "id": "6yFjpiKPBsxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for image normalizing and resinging to standard size\n",
        "datagen = ImageDataGenerator( \n",
        "    rescale=1./255,\n",
        "    rotation_range = 10,\n",
        "    horizontal_flip = True,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    fill_mode = 'nearest')\n",
        "\n",
        "testgen = ImageDataGenerator(rescale=1./255)\n",
        "datagen.fit(X_train)\n",
        "size = 64"
      ],
      "metadata": {
        "id": "FcP83PBl7U8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_flow = datagen.flow(X_train, y_train, batch_size=size) \n",
        "test_flow = testgen.flow(X_test, y_test, batch_size=size)"
      ],
      "metadata": {
        "id": "8ZJniXMxVTSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_FERModel(input_shape=(48,48,1)):\n",
        "    # first input model\n",
        "    visible = Input(shape=input_shape, name='input')\n",
        "    num_classes = 7\n",
        "    #the 1-st block\n",
        "    conv1_1 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_1')(visible)\n",
        "    conv1_1 = BatchNormalization()(conv1_1)\n",
        "    conv1_2 = Conv2D(64, kernel_size=3, activation='relu', padding='same', name = 'conv1_2')(conv1_1)\n",
        "    conv1_2 = BatchNormalization()(conv1_2)\n",
        "    pool1_1 = MaxPooling2D(pool_size=(2,2), name = 'pool1_1')(conv1_2)\n",
        "    drop1_1 = Dropout(0.3, name = 'drop1_1')(pool1_1)#the 2-nd block\n",
        "    conv2_1 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_1')(drop1_1)\n",
        "    conv2_1 = BatchNormalization()(conv2_1)\n",
        "    conv2_2 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_2')(conv2_1)\n",
        "    conv2_2 = BatchNormalization()(conv2_2)\n",
        "    conv2_3 = Conv2D(128, kernel_size=3, activation='relu', padding='same', name = 'conv2_3')(conv2_2)\n",
        "    conv2_2 = BatchNormalization()(conv2_3)\n",
        "    pool2_1 = MaxPooling2D(pool_size=(2,2), name = 'pool2_1')(conv2_3)\n",
        "    drop2_1 = Dropout(0.3, name = 'drop2_1')(pool2_1)#the 3-rd block\n",
        "    conv3_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_1')(drop2_1)\n",
        "    conv3_1 = BatchNormalization()(conv3_1)\n",
        "    conv3_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_2')(conv3_1)\n",
        "    conv3_2 = BatchNormalization()(conv3_2)\n",
        "    conv3_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_3')(conv3_2)\n",
        "    conv3_3 = BatchNormalization()(conv3_3)\n",
        "    conv3_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv3_4')(conv3_3)\n",
        "    conv3_4 = BatchNormalization()(conv3_4)\n",
        "    pool3_1 = MaxPooling2D(pool_size=(2,2), name = 'pool3_1')(conv3_4)\n",
        "    drop3_1 = Dropout(0.3, name = 'drop3_1')(pool3_1)#the 4-th block\n",
        "    conv4_1 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_1')(drop3_1)\n",
        "    conv4_1 = BatchNormalization()(conv4_1)\n",
        "    conv4_2 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_2')(conv4_1)\n",
        "    conv4_2 = BatchNormalization()(conv4_2)\n",
        "    conv4_3 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_3')(conv4_2)\n",
        "    conv4_3 = BatchNormalization()(conv4_3)\n",
        "    conv4_4 = Conv2D(256, kernel_size=3, activation='relu', padding='same', name = 'conv4_4')(conv4_3)\n",
        "    conv4_4 = BatchNormalization()(conv4_4)\n",
        "    pool4_1 = MaxPooling2D(pool_size=(2,2), name = 'pool4_1')(conv4_4)\n",
        "    drop4_1 = Dropout(0.3, name = 'drop4_1')(pool4_1)\n",
        "    \n",
        "    #the 5-th block\n",
        "    conv5_1 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_1')(drop4_1)\n",
        "    conv5_1 = BatchNormalization()(conv5_1)\n",
        "    conv5_2 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_2')(conv5_1)\n",
        "    conv5_2 = BatchNormalization()(conv5_2)\n",
        "    conv5_3 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_3')(conv5_2)\n",
        "    conv5_3 = BatchNormalization()(conv5_3)\n",
        "    conv5_4 = Conv2D(512, kernel_size=3, activation='relu', padding='same', name = 'conv5_4')(conv5_3)\n",
        "    conv5_3 = BatchNormalization()(conv5_3)\n",
        "    pool5_1 = MaxPooling2D(pool_size=(2,2), name = 'pool5_1')(conv5_4)\n",
        "    drop5_1 = Dropout(0.3, name = 'drop5_1')(pool5_1)#Flatten and output\n",
        "    flatten = Flatten(name = 'flatten')(drop5_1)\n",
        "    ouput = Dense(num_classes, activation='softmax', name = 'output')(flatten)# create model \n",
        "    model = Model(inputs =visible, outputs = ouput)\n",
        "    # summary layers\n",
        "    print(model.summary())\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "4auLlnYCV7c5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train_model = CNN_FERModel()\n",
        "optimizer = Adam(lr=0.0001, decay=1e-6)\n",
        "Train_model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "tain_history = Train_model.fit_generator(train_flow, \n",
        "                    steps_per_epoch=len(X_train) / size, \n",
        "                    epochs=100,  \n",
        "                    verbose=1,  \n",
        "                    validation_data=test_flow, validation_steps=len(X_test) / size) \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_ni3WVy6DED",
        "outputId": "fd18b399-e521-414d-ec05-633f0941e0e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input (InputLayer)          [(None, 48, 48, 1)]       0         \n",
            "                                                                 \n",
            " conv1_1 (Conv2D)            (None, 48, 48, 64)        640       \n",
            "                                                                 \n",
            " batch_normalization_34 (Bat  (None, 48, 48, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv1_2 (Conv2D)            (None, 48, 48, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_35 (Bat  (None, 48, 48, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " pool1_1 (MaxPooling2D)      (None, 24, 24, 64)        0         \n",
            "                                                                 \n",
            " drop1_1 (Dropout)           (None, 24, 24, 64)        0         \n",
            "                                                                 \n",
            " conv2_1 (Conv2D)            (None, 24, 24, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_36 (Bat  (None, 24, 24, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2_2 (Conv2D)            (None, 24, 24, 128)       147584    \n",
            "                                                                 \n",
            " batch_normalization_37 (Bat  (None, 24, 24, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2_3 (Conv2D)            (None, 24, 24, 128)       147584    \n",
            "                                                                 \n",
            " pool2_1 (MaxPooling2D)      (None, 12, 12, 128)       0         \n",
            "                                                                 \n",
            " drop2_1 (Dropout)           (None, 12, 12, 128)       0         \n",
            "                                                                 \n",
            " conv3_1 (Conv2D)            (None, 12, 12, 256)       295168    \n",
            "                                                                 \n",
            " batch_normalization_39 (Bat  (None, 12, 12, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv3_2 (Conv2D)            (None, 12, 12, 256)       590080    \n",
            "                                                                 \n",
            " batch_normalization_40 (Bat  (None, 12, 12, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv3_3 (Conv2D)            (None, 12, 12, 256)       590080    \n",
            "                                                                 \n",
            " batch_normalization_41 (Bat  (None, 12, 12, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv3_4 (Conv2D)            (None, 12, 12, 256)       590080    \n",
            "                                                                 \n",
            " batch_normalization_42 (Bat  (None, 12, 12, 256)      1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " pool3_1 (MaxPooling2D)      (None, 6, 6, 256)         0         \n",
            "                                                                 \n",
            " drop3_1 (Dropout)           (None, 6, 6, 256)         0         \n",
            "                                                                 \n",
            " conv4_1 (Conv2D)            (None, 6, 6, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_43 (Bat  (None, 6, 6, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv4_2 (Conv2D)            (None, 6, 6, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_44 (Bat  (None, 6, 6, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv4_3 (Conv2D)            (None, 6, 6, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_45 (Bat  (None, 6, 6, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv4_4 (Conv2D)            (None, 6, 6, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_46 (Bat  (None, 6, 6, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " pool4_1 (MaxPooling2D)      (None, 3, 3, 256)         0         \n",
            "                                                                 \n",
            " drop4_1 (Dropout)           (None, 3, 3, 256)         0         \n",
            "                                                                 \n",
            " conv5_1 (Conv2D)            (None, 3, 3, 512)         1180160   \n",
            "                                                                 \n",
            " batch_normalization_47 (Bat  (None, 3, 3, 512)        2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv5_2 (Conv2D)            (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " batch_normalization_48 (Bat  (None, 3, 3, 512)        2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv5_3 (Conv2D)            (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " batch_normalization_49 (Bat  (None, 3, 3, 512)        2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv5_4 (Conv2D)            (None, 3, 3, 512)         2359808   \n",
            "                                                                 \n",
            " pool5_1 (MaxPooling2D)      (None, 1, 1, 512)         0         \n",
            "                                                                 \n",
            " drop5_1 (Dropout)           (None, 1, 1, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 512)               0         \n",
            "                                                                 \n",
            " output (Dense)              (None, 7)                 3591      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,111,367\n",
            "Trainable params: 13,103,431\n",
            "Non-trainable params: 7,936\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  if __name__ == '__main__':\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "448/448 [==============================] - 34s 72ms/step - loss: 2.0130 - accuracy: 0.2163 - val_loss: 1.8661 - val_accuracy: 0.2499\n",
            "Epoch 2/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 1.7836 - accuracy: 0.2575 - val_loss: 1.7748 - val_accuracy: 0.2795\n",
            "Epoch 3/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 1.7478 - accuracy: 0.2785 - val_loss: 2.1100 - val_accuracy: 0.2641\n",
            "Epoch 4/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 1.6919 - accuracy: 0.3184 - val_loss: 2.0494 - val_accuracy: 0.2923\n",
            "Epoch 5/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 1.6168 - accuracy: 0.3626 - val_loss: 1.5470 - val_accuracy: 0.4110\n",
            "Epoch 6/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 1.5521 - accuracy: 0.3974 - val_loss: 1.5844 - val_accuracy: 0.4177\n",
            "Epoch 7/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 1.4909 - accuracy: 0.4190 - val_loss: 1.4228 - val_accuracy: 0.4452\n",
            "Epoch 8/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 1.4292 - accuracy: 0.4448 - val_loss: 1.3481 - val_accuracy: 0.4734\n",
            "Epoch 9/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 1.3679 - accuracy: 0.4742 - val_loss: 1.3181 - val_accuracy: 0.5046\n",
            "Epoch 10/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 1.3197 - accuracy: 0.4953 - val_loss: 1.2643 - val_accuracy: 0.5163\n",
            "Epoch 11/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 1.2710 - accuracy: 0.5145 - val_loss: 1.2648 - val_accuracy: 0.5235\n",
            "Epoch 12/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 1.2345 - accuracy: 0.5312 - val_loss: 1.2548 - val_accuracy: 0.5249\n",
            "Epoch 13/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 1.2006 - accuracy: 0.5467 - val_loss: 1.2269 - val_accuracy: 0.5238\n",
            "Epoch 14/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 1.1706 - accuracy: 0.5562 - val_loss: 1.1774 - val_accuracy: 0.5603\n",
            "Epoch 15/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 1.1435 - accuracy: 0.5671 - val_loss: 1.1642 - val_accuracy: 0.5645\n",
            "Epoch 16/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 1.1227 - accuracy: 0.5763 - val_loss: 1.1575 - val_accuracy: 0.5553\n",
            "Epoch 17/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 1.0937 - accuracy: 0.5851 - val_loss: 1.1656 - val_accuracy: 0.5651\n",
            "Epoch 18/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 1.0750 - accuracy: 0.5969 - val_loss: 1.1214 - val_accuracy: 0.5765\n",
            "Epoch 19/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 1.0555 - accuracy: 0.6031 - val_loss: 1.0789 - val_accuracy: 0.5926\n",
            "Epoch 20/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 1.0418 - accuracy: 0.6090 - val_loss: 1.0721 - val_accuracy: 0.6113\n",
            "Epoch 21/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 1.0200 - accuracy: 0.6144 - val_loss: 1.0607 - val_accuracy: 0.5890\n",
            "Epoch 22/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 1.0025 - accuracy: 0.6230 - val_loss: 1.0604 - val_accuracy: 0.6004\n",
            "Epoch 23/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.9887 - accuracy: 0.6266 - val_loss: 1.0290 - val_accuracy: 0.6169\n",
            "Epoch 24/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.9709 - accuracy: 0.6367 - val_loss: 1.0535 - val_accuracy: 0.5985\n",
            "Epoch 25/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.9589 - accuracy: 0.6409 - val_loss: 0.9931 - val_accuracy: 0.6261\n",
            "Epoch 26/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.9423 - accuracy: 0.6454 - val_loss: 1.0283 - val_accuracy: 0.6225\n",
            "Epoch 27/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.9275 - accuracy: 0.6517 - val_loss: 1.0578 - val_accuracy: 0.6166\n",
            "Epoch 28/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.9186 - accuracy: 0.6567 - val_loss: 1.0029 - val_accuracy: 0.6264\n",
            "Epoch 29/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.9001 - accuracy: 0.6617 - val_loss: 1.0157 - val_accuracy: 0.6328\n",
            "Epoch 30/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.8916 - accuracy: 0.6659 - val_loss: 0.9686 - val_accuracy: 0.6442\n",
            "Epoch 31/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.8830 - accuracy: 0.6677 - val_loss: 1.0571 - val_accuracy: 0.6152\n",
            "Epoch 32/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.8668 - accuracy: 0.6763 - val_loss: 0.9602 - val_accuracy: 0.6481\n",
            "Epoch 33/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.8520 - accuracy: 0.6821 - val_loss: 0.9577 - val_accuracy: 0.6442\n",
            "Epoch 34/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.8361 - accuracy: 0.6888 - val_loss: 0.9439 - val_accuracy: 0.6612\n",
            "Epoch 35/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.8278 - accuracy: 0.6905 - val_loss: 0.9944 - val_accuracy: 0.6303\n",
            "Epoch 36/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.8170 - accuracy: 0.6937 - val_loss: 0.9919 - val_accuracy: 0.6372\n",
            "Epoch 37/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.8010 - accuracy: 0.6994 - val_loss: 0.9816 - val_accuracy: 0.6553\n",
            "Epoch 38/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 0.7932 - accuracy: 0.7018 - val_loss: 0.9727 - val_accuracy: 0.6584\n",
            "Epoch 39/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.7858 - accuracy: 0.7083 - val_loss: 0.9874 - val_accuracy: 0.6506\n",
            "Epoch 40/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.7744 - accuracy: 0.7114 - val_loss: 0.9730 - val_accuracy: 0.6531\n",
            "Epoch 41/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.7643 - accuracy: 0.7126 - val_loss: 0.9828 - val_accuracy: 0.6562\n",
            "Epoch 42/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.7483 - accuracy: 0.7242 - val_loss: 0.9734 - val_accuracy: 0.6698\n",
            "Epoch 43/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.7422 - accuracy: 0.7246 - val_loss: 0.9921 - val_accuracy: 0.6539\n",
            "Epoch 44/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.7301 - accuracy: 0.7253 - val_loss: 0.9873 - val_accuracy: 0.6553\n",
            "Epoch 45/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.7139 - accuracy: 0.7323 - val_loss: 1.0033 - val_accuracy: 0.6590\n",
            "Epoch 46/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.7079 - accuracy: 0.7373 - val_loss: 1.0064 - val_accuracy: 0.6475\n",
            "Epoch 47/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.6905 - accuracy: 0.7422 - val_loss: 1.0397 - val_accuracy: 0.6512\n",
            "Epoch 48/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.6895 - accuracy: 0.7434 - val_loss: 0.9823 - val_accuracy: 0.6676\n",
            "Epoch 49/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.6757 - accuracy: 0.7469 - val_loss: 0.9681 - val_accuracy: 0.6695\n",
            "Epoch 50/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.6651 - accuracy: 0.7504 - val_loss: 0.9918 - val_accuracy: 0.6715\n",
            "Epoch 51/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.6571 - accuracy: 0.7569 - val_loss: 1.0238 - val_accuracy: 0.6509\n",
            "Epoch 52/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.6486 - accuracy: 0.7589 - val_loss: 0.9980 - val_accuracy: 0.6729\n",
            "Epoch 53/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.6422 - accuracy: 0.7623 - val_loss: 0.9798 - val_accuracy: 0.6854\n",
            "Epoch 54/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 0.6234 - accuracy: 0.7668 - val_loss: 1.0257 - val_accuracy: 0.6709\n",
            "Epoch 55/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.6148 - accuracy: 0.7724 - val_loss: 1.0456 - val_accuracy: 0.6676\n",
            "Epoch 56/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.6083 - accuracy: 0.7755 - val_loss: 1.0444 - val_accuracy: 0.6723\n",
            "Epoch 57/100\n",
            "448/448 [==============================] - 33s 72ms/step - loss: 0.5972 - accuracy: 0.7786 - val_loss: 1.0550 - val_accuracy: 0.6701\n",
            "Epoch 58/100\n",
            "448/448 [==============================] - 33s 72ms/step - loss: 0.5881 - accuracy: 0.7820 - val_loss: 1.0633 - val_accuracy: 0.6695\n",
            "Epoch 59/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 0.5785 - accuracy: 0.7875 - val_loss: 1.0425 - val_accuracy: 0.6581\n",
            "Epoch 60/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 0.5727 - accuracy: 0.7866 - val_loss: 1.0282 - val_accuracy: 0.6818\n",
            "Epoch 61/100\n",
            "448/448 [==============================] - 33s 72ms/step - loss: 0.5605 - accuracy: 0.7930 - val_loss: 1.1005 - val_accuracy: 0.6604\n",
            "Epoch 62/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 0.5530 - accuracy: 0.7989 - val_loss: 1.0701 - val_accuracy: 0.6648\n",
            "Epoch 63/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 0.5488 - accuracy: 0.7969 - val_loss: 1.0503 - val_accuracy: 0.6737\n",
            "Epoch 64/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 0.5388 - accuracy: 0.7990 - val_loss: 1.0834 - val_accuracy: 0.6768\n",
            "Epoch 65/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 0.5199 - accuracy: 0.8067 - val_loss: 1.1351 - val_accuracy: 0.6757\n",
            "Epoch 66/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 0.5231 - accuracy: 0.8064 - val_loss: 1.0667 - val_accuracy: 0.6812\n",
            "Epoch 67/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.5159 - accuracy: 0.8093 - val_loss: 1.0886 - val_accuracy: 0.6740\n",
            "Epoch 68/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 0.5001 - accuracy: 0.8132 - val_loss: 1.1479 - val_accuracy: 0.6687\n",
            "Epoch 69/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 0.4979 - accuracy: 0.8136 - val_loss: 1.1022 - val_accuracy: 0.6679\n",
            "Epoch 70/100\n",
            "448/448 [==============================] - 33s 74ms/step - loss: 0.4894 - accuracy: 0.8175 - val_loss: 1.1480 - val_accuracy: 0.6654\n",
            "Epoch 71/100\n",
            "448/448 [==============================] - 33s 72ms/step - loss: 0.4817 - accuracy: 0.8209 - val_loss: 1.1845 - val_accuracy: 0.6740\n",
            "Epoch 72/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 0.4705 - accuracy: 0.8279 - val_loss: 1.1251 - val_accuracy: 0.6721\n",
            "Epoch 73/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.4591 - accuracy: 0.8311 - val_loss: 1.1616 - val_accuracy: 0.6754\n",
            "Epoch 74/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.4581 - accuracy: 0.8330 - val_loss: 1.1370 - val_accuracy: 0.6648\n",
            "Epoch 75/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 0.4531 - accuracy: 0.8318 - val_loss: 1.1322 - val_accuracy: 0.6796\n",
            "Epoch 76/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.4384 - accuracy: 0.8364 - val_loss: 1.1581 - val_accuracy: 0.6709\n",
            "Epoch 77/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.4408 - accuracy: 0.8355 - val_loss: 1.2264 - val_accuracy: 0.6687\n",
            "Epoch 78/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.4353 - accuracy: 0.8386 - val_loss: 1.1952 - val_accuracy: 0.6721\n",
            "Epoch 79/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.4213 - accuracy: 0.8457 - val_loss: 1.2473 - val_accuracy: 0.6771\n",
            "Epoch 80/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.4103 - accuracy: 0.8484 - val_loss: 1.2222 - val_accuracy: 0.6712\n",
            "Epoch 81/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.4056 - accuracy: 0.8492 - val_loss: 1.2674 - val_accuracy: 0.6718\n",
            "Epoch 82/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.4085 - accuracy: 0.8484 - val_loss: 1.2645 - val_accuracy: 0.6715\n",
            "Epoch 83/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.3959 - accuracy: 0.8519 - val_loss: 1.2593 - val_accuracy: 0.6768\n",
            "Epoch 84/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.3878 - accuracy: 0.8560 - val_loss: 1.2861 - val_accuracy: 0.6673\n",
            "Epoch 85/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.3860 - accuracy: 0.8575 - val_loss: 1.2668 - val_accuracy: 0.6793\n",
            "Epoch 86/100\n",
            "448/448 [==============================] - 33s 73ms/step - loss: 0.3768 - accuracy: 0.8596 - val_loss: 1.3210 - val_accuracy: 0.6704\n",
            "Epoch 87/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.3698 - accuracy: 0.8622 - val_loss: 1.3632 - val_accuracy: 0.6746\n",
            "Epoch 88/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.3699 - accuracy: 0.8624 - val_loss: 1.2938 - val_accuracy: 0.6670\n",
            "Epoch 89/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.3576 - accuracy: 0.8683 - val_loss: 1.3161 - val_accuracy: 0.6773\n",
            "Epoch 90/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.3510 - accuracy: 0.8701 - val_loss: 1.3287 - val_accuracy: 0.6762\n",
            "Epoch 91/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.3511 - accuracy: 0.8712 - val_loss: 1.3504 - val_accuracy: 0.6746\n",
            "Epoch 92/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.3473 - accuracy: 0.8713 - val_loss: 1.3847 - val_accuracy: 0.6732\n",
            "Epoch 93/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.3409 - accuracy: 0.8735 - val_loss: 1.3808 - val_accuracy: 0.6790\n",
            "Epoch 94/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.3305 - accuracy: 0.8806 - val_loss: 1.3940 - val_accuracy: 0.6751\n",
            "Epoch 95/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.3298 - accuracy: 0.8784 - val_loss: 1.3652 - val_accuracy: 0.6807\n",
            "Epoch 96/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.3249 - accuracy: 0.8803 - val_loss: 1.4039 - val_accuracy: 0.6762\n",
            "Epoch 97/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.3237 - accuracy: 0.8805 - val_loss: 1.3357 - val_accuracy: 0.6768\n",
            "Epoch 98/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.3144 - accuracy: 0.8852 - val_loss: 1.4405 - val_accuracy: 0.6737\n",
            "Epoch 99/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.3073 - accuracy: 0.8868 - val_loss: 1.3945 - val_accuracy: 0.6815\n",
            "Epoch 100/100\n",
            "448/448 [==============================] - 32s 72ms/step - loss: 0.3010 - accuracy: 0.8878 - val_loss: 1.3832 - val_accuracy: 0.6824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tain_history.history['loss']\n",
        "val_loss = tain_history.history['val_loss']\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, 'y', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "kYVTrXXvAYKq",
        "outputId": "85584461-0ffe-4495-ac81-c058a3e69b7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_trained = Train_model.to_json()\n",
        "with open(\"Train_model.json\", \"w\") as json_file:\n",
        "    json_file.write(model_trained)\n",
        "Train_model.save_weights(\"model.h5\")\n",
        "    \n",
        "Train_model.save(\"model.h5\")"
      ],
      "metadata": {
        "id": "M2F3bP29c0tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train_model.save(\"trained_model.h5\")"
      ],
      "metadata": {
        "id": "04d3STjcjFZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CvNA6MoCkHeH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}